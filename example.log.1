WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [7LkWCbpqQVGgIAT98iP_8w][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [7LkWCbpqQVGgIAT98iP_8w][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13094], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/ctFggnQMRbSLvigT5UJPLg
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{ctFggnQMRbSLvigT5UJPLg}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5c8uv0plytel|73041b7d, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5c8uv0plytel|73041b7d, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [ctFggnQMRbSLvigT5UJPLg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [ctFggnQMRbSLvigT5UJPLg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [ctFggnQMRbSLvigT5UJPLg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13117], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/0vaXcrxASwa3p9osJgh4Og
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{0vaXcrxASwa3p9osJgh4Og}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5cbj1o1xnycfd|1235151c, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5cbj1o1xnycfd|1235151c, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [0vaXcrxASwa3p9osJgh4Og][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13262], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/Uyq9MO2pRJS8DvcpMXOeJw
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{Uyq9MO2pRJS8DvcpMXOeJw}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5cklit1x6gsi9|73041b7d, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5cklit1x6gsi9|73041b7d, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [Uyq9MO2pRJS8DvcpMXOeJw][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13300], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/7BRJ7uVfSGSiPKFWd35yng
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{7BRJ7uVfSGSiPKFWd35yng}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5clxry1g0v8qu|73041b7d, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5clxry1g0v8qu|73041b7d, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [7BRJ7uVfSGSiPKFWd35yng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13326], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/CK_tP9tZQtayVP3gusoWyw
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{CK_tP9tZQtayVP3gusoWyw}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5cn2d31xnpadc|5cd96b41, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5cn2d31xnpadc|5cd96b41, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CK_tP9tZQtayVP3gusoWyw][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13355], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/eLAj5mVhQlO2M1IN60Uq8A
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{eLAj5mVhQlO2M1IN60Uq8A}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5coa6f1vlz41s|5cd96b41, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5coa6f1vlz41s|5cd96b41, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.2gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [eLAj5mVhQlO2M1IN60Uq8A][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.2gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13390], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.2gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/orwVLXqgQM-0JMn7ghyKLQ
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{orwVLXqgQM-0JMn7ghyKLQ}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5ctcviv390ol|73041b7d, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5ctcviv390ol|73041b7d, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [orwVLXqgQM-0JMn7ghyKLQ][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.2gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13482], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/CWuz6FRITO6UItrgZQJa2g
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{CWuz6FRITO6UItrgZQJa2g}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5cy9dg1j5s70v|73041b7d, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5cy9dg1j5s70v|73041b7d, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.4gb[4.8%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [CWuz6FRITO6UItrgZQJa2g][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.4gb[4.8%], shards will be relocated away from this node
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13589], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/TJYcAttQTAeLo9eKOw9qjg
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{TJYcAttQTAeLo9eKOw9qjg}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5d8y6l1l2hw92|1235151c, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5d8y6l1l2hw92|1235151c, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [TJYcAttQTAeLo9eKOw9qjg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [TJYcAttQTAeLo9eKOw9qjg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [TJYcAttQTAeLo9eKOw9qjg][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13615], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/KhpuamMEQTi8Bx_QqVkl9w
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{KhpuamMEQTi8Bx_QqVkl9w}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5dbg9w17duaum|7c5c20ed, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5dbg9w17duaum|7c5c20ed, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [KhpuamMEQTi8Bx_QqVkl9w][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13640], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/vG1Ee5nIQPytIi8NyTCUcA
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{vG1Ee5nIQPytIi8NyTCUcA}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5dcrom1f4pm8r|79604abe, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5dcrom1f4pm8r|79604abe, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [vG1Ee5nIQPytIi8NyTCUcA][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [vG1Ee5nIQPytIi8NyTCUcA][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
INFO main org.elasticsearch.node - [mws-rest-api] version[2.3.3], pid[13698], build[218bdf1/2016-05-17T15:40:04Z]
INFO main org.elasticsearch.node - [mws-rest-api] initializing ...
INFO main org.elasticsearch.plugins - [mws-rest-api] modules [], plugins [], sites []
INFO main org.elasticsearch.env - [mws-rest-api] using [1] data paths, mounts [[/ (/dev/disk2)]], net usable_space [11.1gb], net total_space [237.3gb], spins? [unknown], types [hfs]
INFO main org.elasticsearch.env - [mws-rest-api] heap size [1.7gb], compressed ordinary object pointers [true]
WARN main org.elasticsearch.env - [mws-rest-api] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to at least [65536]
INFO main org.elasticsearch.node - [mws-rest-api] initialized
INFO main org.elasticsearch.node - [mws-rest-api] starting ...
INFO main org.elasticsearch.transport - [mws-rest-api] publish_address {local[1]}, bound_addresses {local[1]}
INFO main org.elasticsearch.discovery - [mws-rest-api] elasticsearch/FHpXNqDcSnmF7lYg2ew6Ng
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.cluster.service - [mws-rest-api] new_master {mws-rest-api}{FHpXNqDcSnmF7lYg2ew6Ng}{local}{local[1]}{local=true}, reason: local-disco-initial_connect(master)
INFO elasticsearch[mws-rest-api][clusterService#updateTask][T#1] org.elasticsearch.gateway - [mws-rest-api] recovered [0] indices into cluster_state
INFO main org.elasticsearch.http - [mws-rest-api] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
INFO main org.elasticsearch.node - [mws-rest-api] started
INFO MLog-Init-Reporter com.mchange.v2.log.MLog - MLog clients using log4j logging.
INFO main com.mchange.v2.c3p0.C3P0Registry - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]
INFO vertx-jdbc-service-get-connection-thread com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource - Initializing c3p0 pool... com.mchange.v2.c3p0.ComboPooledDataSource [ acquireIncrement -> 3, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, dataSourceName -> 1hge13g9j5dhy1ehf1zjr|5cd96b41, debugUnreturnedConnectionStackTraces -> false, description -> null, driverClass -> org.h2.Driver, extensions -> {}, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, forceUseNamedDriverClass -> false, identityToken -> 1hge13g9j5dhy1ehf1zjr|5cd96b41, idleConnectionTestPeriod -> 0, initialPoolSize -> 3, jdbcUrl -> jdbc:h2:/Users/zaro/dev/mws-rest-api/db/presets.db, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 30, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 3, numHelperThreads -> 3, preferredTestQuery -> null, privilegeSpawnedThreads -> false, properties -> {}, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, userOverrides -> {}, usesTraditionalReflectiveProxies -> false ]
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.7%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11.1gb[4.6%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 11gb[4.6%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
INFO elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] rerouting shards: [high disk watermark exceeded on one or more nodes]
WARN elasticsearch[mws-rest-api][management][T#1] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
WARN elasticsearch[mws-rest-api][management][T#2] org.elasticsearch.cluster.routing.allocation.decider - [mws-rest-api] high disk watermark [90%] exceeded on [FHpXNqDcSnmF7lYg2ew6Ng][mws-rest-api][/Users/zaro/dev/mws-rest-api/db/data/elasticsearch/nodes/0] free: 10.8gb[4.5%], shards will be relocated away from this node
